\documentclass[ILAS2025-program.tex]{subfiles}

\begin{document}

\parindent=0pt

\section{Abstracts of Plenary Talks}
 \hypertarget{down0009}{}\begin{ilasabstract}
   \talktitle{nan}
    
    \textbf{Haim Avron}, \info{11:00\textrm{--}12:00 @ SYS Hall (June 27, Friday)} \hfill \hyperlink{up0009}{$\Uparrow$}
    
    \mtskip
    nan\end{ilasabstract}
     \hypertarget{down0005}{}\begin{ilasabstract}
   \talktitle{Matrix structures in queueing and network models: An overview}
    
    \textbf{Dario Andrea Bini}, \info{09:00\textrm{--}10:00 @ SYS Hall (June 25, Wednesday)} \hfill \hyperlink{up0005}{$\Uparrow$}
    
    \mtskip
    We provide an overview of some results concerning classes of matrices involved in queueing and network models. We show specific examples where the analysis of matrix structures, besides providing a better understanding of the original problem, is a fundamental step in designing effective ad hoc solution algorithms.

Two specific issues are considered:  the analysis of random walks on a regular grid in the quarter plane, and the assessment of the centrality of the edges in a graph. Both issues derive from the analysis of relevant real-world problems and are modeled by Markov chains describing random walks on a graph. 

In the first topic, the graph is a regular grid, and the specific features of the problem lead to semi-infinite transition probability matrices with a two-level tridiagonal and almost Toeplitz structure. We show that these matrices live in a suitable infinite-dimensional structured matrix algebra which is also a Banach space with respect to the infinity norm. This fact will be crucial to designing effective fast algorithms for computing the steady-state vector of the associated Markov chain.

In the second topic, the graph is typically a road map of a city or region. Thus, the Toeplitz structure of the associated transition probability matrix is lost. However, the sparsity and band structure of this matrix will allow us to easily compute a centrality measure of the edges defined in terms of Kemeny's constant of the associated Markov chain. The effectiveness of the model and the solution algorithms is tested on the road maps of Pisa and Tuscany.
\end{ilasabstract}
     \hypertarget{down0006}{}\begin{ilasabstract}
   \talktitle{Clustering in graphs with high clustering coefficients}
    
    \textbf{Fan Chung}, \info{08:00\textrm{--}09:00 @ SYS Hall (June 26, Thursday)} \hfill \hyperlink{up0006}{$\Uparrow$}
    
    \mtskip
    Many real world networks possess the so-called small world phenomenon where every node is relatively close to every other node and have a large clustering coefficient, i.e., friends of friends are likely friends. The task of learning an adequate similarity measure on various feature spaces often involves  graphs with high clustering coefficients.
We investigate the clustering effect in sparse  clustering graphs by examining the structural and spectral  properties as well as the enumeration  of patterns. In addition, we consider random graph models for clustering graphs that can be use to analyze the behavior of complex networks.\end{ilasabstract}
     \hypertarget{down0003}{}\begin{ilasabstract}
   \talktitle{The canonical form for congruence: some history and applications}
    
    \textbf{Fernando De TerÃ¡n}, \info{09:00\textrm{--}10:00 @ SYS Hall (June 24, Tuesday)} \hfill \hyperlink{up0003}{$\Uparrow$}
    
    \mtskip
    A square matrix $A$ can be either seen as a linear map or as a bilinear form. When considered as a linear map, it is natural to introduce the relation of ``similarity'', $P^{-1}AP$ (with $P$ invertible), which is a change of basis that allows us to represent the linear map in a simpler form, in particular in the well-known ``Jordan canonical form''. When considering the matrix $A$ as a bilinear form, the natural relation instead is the one of ``congruence'', $P^\top AP$ (with $P$ invertible), which is the suitable change of basis for bilinear forms. Is there a canonical form for such a relation? The answer is yes, and actually some different canonical forms for congruence have been introduced over the years since the 1930's. In this talk I will introduce the most recent one (by Horn and Sergeichuk, 2006), review the history of this and the other canonical forms, and show some applications in the context of my past and current research, including:
\begin{itemize}
    \item The solution of the equation $AX+X^\top A=0$ and its connection to $\top$-palindromic pencils.
    \item The consistency of the equation $X^\top AX=B$ when $B$ is either symmetric or skew-symmetric.
\end{itemize}\end{ilasabstract}
     \hypertarget{down0001}{}\begin{ilasabstract}
   \talktitle{Various inequalities for quasi-arithmetic mean and quasi-geometric type means of matrices}
    
    \textbf{Fumio Hiai}, \info{09:30\textrm{--}10:30 @ SYS Hall (June 23, Monday)} \hfill \hyperlink{up0001}{$\Uparrow$}
    
    \mtskip
    Our targets in this talk are the quasi extensions of the weighted arithmetic mean and of the weighted geometric type means, including the weighted geometric mean, the weighted spectral geometric mean, the R\'enyi mean, and the Log-Euclidean mean, for positive (semi)definite matrices. For example,
\begin{align*}
&\mathcal{A}_{\alpha,p}(A,B):=(\alpha A^p+(1-\alpha)B^p)^{1/p},
\ \mbox{the quasi-weighted arithmetic mean}, \\
&G_{\alpha,p}(A,B):=(A^p\#_\alpha B^p)^{1/p},
\ \mbox{the quasi-weighted geometric mean}, \\
&R_{\alpha,p}(A,B):=\bigl(B^{{\frac{1-\alpha}{2}}p}A^{\alpha p}B^{{\frac{1-\alpha}{2}}p}\bigr)^{1/p},
\ \mbox{the R\'enyi mean}, \\
&LE_\alpha(A,B):=\exp(\alpha\log A+(1-\alpha)\log B),
\ \mbox{the weighted Log-Euclidean mean},
\end{align*}
where $\alpha\in(0,1)$ (or all $\alpha>0$) is the weight parameter and $p>0$ is the parameter of the quasi extension.
For the quasi versions of these weighted matrix means we consider inequalities in different types of orders such as the Loewner order $\le$, the chaotic $\le_{\mathrm{ch}}$ (i.e., $\log X\le \log Y$), the near order $\le_{\mathrm{ne}}$ (i.e., $X\#Y^{-1}\le I$), the entrywise eigenvalue order $\le_\lambda$, the log-majorization $\prec_{\log}$, the weak majorization $\prec_w$, and the order under trace (i.e., $\mathrm{Tr}\,X\le\mathrm{Tr}\,Y$), whose order relations weaken in this writing order.
Our objective is to pursue under which condition of the parameters $\alpha,p,q$ the inequality $\mathcal{M}_{\alpha,p}(A,B)\triangleleft\mathcal{N}_{\alpha,q}(A,B)$ holds for all positive (semi)definite matrices $A,B$, for any pair $(\mathcal{M}_{\alpha,p},\mathcal{N}_{\alpha,q})$ from the above quasi-weighted means and for any order $\triangleleft$ mentioned above.
For instance, it is shown that $\mathcal{A}_{\alpha,p}(A,B)\le\mathcal{A}_{\alpha,q}(A,B)$ holds for all $A,B\ge0$ if and only if $p=q$ or $1\le p<q$ or $1/2\le p<1\le q$, whereas, for any other weaker order $\triangleleft$, $\mathcal{A}_{\alpha,p}(A,B)\triangleleft\mathcal{A}_{\alpha,q}(A,B)$ holds for all $A,B\ge0$ if and only if $p\le q$. When $(\mathcal{M}_{\alpha,p},\mathcal{N}_{\alpha,q})$ is a pair from the quasi-weighted geometric type means, we are mostly interested in the condition of $\alpha,p,q$ under which the log-majorization $\mathcal{M}_{\alpha,p}(A,B)\prec_{\log}\mathcal{N}_{\alpha,q}(A,B)$ holds and whether $AB=BA$ follows from the equality case $\mathrm{Tr}\,\mathcal{M}_{\alpha,p}(A,B)=\mathrm{Tr}\,\mathcal{N}_{\alpha,q}(A,B)$ in this situation.\end{ilasabstract}
     \hypertarget{down0004}{}\begin{ilasabstract}
   \talktitle{Adaptive randomized pivoting}
    
    \textbf{Daniel Kressner}, \info{08:00\textrm{--}09:00 @ SYS Hall (June 25, Wednesday)} \hfill \hyperlink{up0004}{$\Uparrow$}
    
    \mtskip
    Finding good subsets of row and column indices, often called pivots, is a ubiquitous task in applied and numerical linear algebra. One of its most famous appearances is arguably in Gaussian elimination for solving linear systems, where a good choice of pivots is crucial for numerical stability. This talk will focus on pivoting in the context of low-dimensional approximation, including column subset selection, discrete empirical interpolation, and various interpolative decompositions, such as the CUR and Cholesky/Nystrom approximations. In all of these cases, a greedy choice of pivots usually works well but there are well-known counterexamples where such a choice leads to poor results. We present a new randomized pivot selection strategy that avoids such unfavorable worst-case performance by using adaptivity in two senses: It adapts to information on the range / co-range of a matrix, and the sampling distribution is updated after each pivot selection. Adaptive randomized pivoting enjoys error guarantees that match, in expectation, the best known existence results. At the same time, it is   simpler and usually cheaper than volume-based techniques, which achieve similar guarantees through volume sampling or iterative local volume maximization. We will illustrate several applications of adaptive randomized pivoting and discuss derandomized variants. This talk is based on joint work with Alice Cortinovis, University of Pisa.\end{ilasabstract}
     \hypertarget{down0008}{}\begin{ilasabstract}
   \talktitle{nan}
    
    \textbf{Ren-Cang Li}, \info{10:00\textrm{--}11:00 @ SYS Hall (June 27, Friday)} \hfill \hyperlink{up0008}{$\Uparrow$}
    
    \mtskip
    nan\end{ilasabstract}
     \hypertarget{down0007}{}\begin{ilasabstract}
   \talktitle{nan}
    
    \textbf{Karen Meagher}, \info{09:00\textrm{--}10:00 @ SYS Hall (June 26, Thursday)} \hfill \hyperlink{up0007}{$\Uparrow$}
    
    \mtskip
    nan\end{ilasabstract}
     \hypertarget{down0002}{}\begin{ilasabstract}
   \talktitle{nan}
    
    \textbf{Polona Oblak}, \info{08:00\textrm{--}09:00 @ SYS Hall (June 24, Tuesday)} \hfill \hyperlink{up0002}{$\Uparrow$}
    
    \mtskip
    nan\end{ilasabstract}
     \hypertarget{down0000}{}\begin{ilasabstract}
   \talktitle{A ramble through mathematics relevant for quantum theory: \\  A personal perspective
}
    
    \textbf{Karol Å»yczkowski}, \info{08:30\textrm{--}09:30 @ SYS Hall (June 23, Monday)} \hfill \hyperlink{up0000}{$\Uparrow$}
    
    \mtskip
    Which branches of mathematics are the most important for a physicist working in quantum theory? 
How to foster an interdisciplinary collaboration of a theoretical physicists and a mathematician?
Which problems motivated by quantum theory can be inspiring for the mathematical community?  
Basing on personal experience concerning research related to linear algebra, operator theory, matrix analysis, random matrices, geometry of convex sets, group theory and combinatorics, I will argue that such a collaboration is possible and can be fruitful.
\end{ilasabstract}
    \newpage

\section{Abstracts of Mini-symposium Talks}
 \hypertarget{down0179}{}\begin{ilasabstract}
   \talktitle{Recent developments of switching methods for the construction of cospectral graphs}
    
    \textbf{Aida Abiad}, \info{14:30\textrm{--}15:00 @ SC1005 (June 24, Tuesday)} \hfill \hyperlink{up0179}{$\Uparrow$}
    
    (in {\color{mstitle}MS17: Graphs and matrices in honor of Leslie Hogben's retirement})
        
        \mtskip
    Switching is an operation on a graph that does not change the spectrum of the adjacency matrix, thus producing cospectral graphs. An important activity in the field of spectral graph theory is the characterization of graphs by their spectrum. Hence, switching provides a tool for disproving the existence of such a characterization. 

In this talk we will overview recent progress on switching methods for the construction of cospectral graphs. Work by Wang and Xu (2010) suggests that most cospectral graphs with cospectral complements can be constructed using regular orthogonal matrices of level 2, which has relevance for Haemers' conjecture. In this direction, we will present two new switching methods based on regular orthogonal matrices of level 2. We will also show a general framework for counting the number of graphs that have a non-isomorphic cospectral graph through any of the existing switching methods for the adjacency matrix, expanding on the work by Haemers and Spence (2004).  

This is joint work with Nils van de Berg and Robin Simoens.
\end{ilasabstract}
     \hypertarget{down0059}{}\begin{ilasabstract}
   \talktitle{A low-complexity LSTM network to realize multibeam beamforming
}
    
    \textbf{Hansaka Aluvihare}, \info{14:30\textrm{--}15:00 @ SC1003 (June 23, Monday)} \hfill \hyperlink{up0059}{$\Uparrow$}
    
    (in {\color{mstitle}MS26: Utilizing structure to achieve low-complexity algorithms for data science, engineering, and physics})
        
        \mtskip
    Even large amounts of data can be efficiently realized by imposing structures into neural networks through structured weight matrices. These structured weight matrices could be trained to realize phase shifts for specific beams in multi-beam beamforming, along with input and output vectors made up of time-domain signals. In our previous research, we showed that wideband multi-beam beamformers using true-time-delays (TTDs) can be represented by delay Vandermonde matrices (DVM). We use a frequency-domain transformation to explicitly express the TTD-based time delay data in terms of the elements of the DVM-structured weight matrices. Building upon these weight matrices, we introduce a novel low-complexity neural network LSTM architecture for realizing wideband multi-beam beamformers. The proposed structured LSTM network reduces the computational complexity for realizing wideband multi-beam beamformers from $\mathcal{O}(N^2L)$ to $\mathcal{O}(N^sL)$, where $1 < s < 2$ and $L$ is the number of layers.
\end{ilasabstract}
     \hypertarget{down0170}{}\begin{ilasabstract}
   \talktitle{Doubly stochastic matrices and graphs}
    
    \textbf{Enide Andrade}, \info{14:00\textrm{--}14:30 @ SC1001 (June 24, Tuesday)} \hfill \hyperlink{up0170}{$\Uparrow$}
    
    (in {\color{mstitle}MS21: Linear algebra techniques in graph theory})
        
        \mtskip
    In this talk we present a study about inverses of modified Laplacian matrices; the modification is by adding the
identity matrix which gives a positive definite matrix. We investigate the relationship between
the underlying graph and the properties of this inverse. 

%Some related questions are also
%. The questions and objects are of interest to the matrix theory community and may also
%be of interest in applications involving Laplacians.



\end{ilasabstract}
     \hypertarget{down0338}{}\begin{ilasabstract}
   \talktitle{Laplacian eigenvalues of weighted threshold graphs}
    
    \textbf{Milica AnÄeliÄ}, \info{15:00\textrm{--}15:30 @ SC2001 (June 26, Thursday)} \hfill \hyperlink{up0338}{$\Uparrow$}
    
    (in {\color{mstitle}MS21: Linear algebra techniques in graph theory})
        
        \mtskip
    We provide a closed  formula to compute the Laplacian spectrum of weighted threshold graphs. We also  show that their Laplacian eigenvalues are the parts of the conjugate partition of the associated   weighted Ferrers diagrams. \\

({\it This is a joint work with Zoran Stani\'c, Faculty of Mathematics, University of Belgrade, Serbia.})\end{ilasabstract}
     \hypertarget{down0210}{}\begin{ilasabstract}
   \talktitle{Identifying and estimating dynamical covariance matrices with hierarchical rank structure}
    
    \textbf{Robin Armstrong}, \info{16:30\textrm{--}17:00 @ SC0014 (June 24, Tuesday)} \hfill \hyperlink{up0210}{$\Uparrow$}
    
    (in {\color{mstitle}MS6: Model reduction})
        
        \mtskip
    Many algorithms in data assimilation and model reduction rely on sample-based estimates for covariance matrices associated with the trajectory of a high-dimensional dynamical system. The number of available samples is often far less than the dimension of the underlying state space, making it necessary to impose a regularizing structural assumption such as spatial localization. This talk will examine the use of hierarchical rank structure as a regularizing assumption for high-dimensional covariance estimation. Whereas spatial localization assumes that long-range correlations are near-zero, hierarchical rank structure corresponds to the situation where long-range correlations vary more smoothly than short-range ones. We will first examine, from theoretical and empirical circumstances, the conditions under which this assumption is appropriate. We will then present algorithms and experiments which show how to estimate a high-dimensional covariance matrix from limited samples by imposing hierarchical rank structure. Covariance matrices associated with turbulent fluid dynamics, numerical weather prediction models, and Lorenz-type systems will serve as illustrative examples throughout.
\end{ilasabstract}
     \hypertarget{down0159}{}\begin{ilasabstract}
   \talktitle{nan}
    
    \textbf{Athul Augustine}, \info{14:30\textrm{--}15:00 @ SC0009 (June 24, Tuesday)} \hfill \hyperlink{up0159}{$\Uparrow$}
    
    (in {\color{mstitle}MS29: Matrix functions and related topics})
        
        \mtskip
    nan\end{ilasabstract}
     \hypertarget{down0344}{}\begin{ilasabstract}
   \talktitle{nan}
    
    \textbf{Oleg Balabanov}, \info{14:00\textrm{--}14:30 @ SC3001 (June 26, Thursday)} \hfill \hyperlink{up0344}{$\Uparrow$}
    
    (in {\color{mstitle}MS16: Approximations and errors in Krylov-based solvers})
        
        \mtskip
    nan\end{ilasabstract}
     \hypertarget{down0381}{}\begin{ilasabstract}
   \talktitle{nan}
    
    \textbf{Anirban Banerjee}, \info{17:00\textrm{--}17:30 @ SC2001 (June 26, Thursday)} \hfill \hyperlink{up0381}{$\Uparrow$}
    
    (in {\color{mstitle}MS21: Linear algebra techniques in graph theory})
        
        \mtskip
    nan\end{ilasabstract}
     \hypertarget{down0336}{}\begin{ilasabstract}
   \talktitle{nan}
    
    \textbf{Sasmita Barik}, \info{14:00\textrm{--}14:30 @ SC2001 (June 26, Thursday)} \hfill \hyperlink{up0336}{$\Uparrow$}
    
    (in {\color{mstitle}MS21: Linear algebra techniques in graph theory})
        
        \mtskip
    nan\end{ilasabstract}
     \hypertarget{down0211}{}\begin{ilasabstract}
   \talktitle{Gaussian process regression for the identification of model dynamics}
    
    \textbf{Christopher Beattie}, \info{17:00\textrm{--}17:30 @ SC0014 (June 24, Tuesday)} \hfill \hyperlink{up0211}{$\Uparrow$}
    
    (in {\color{mstitle}MS6: Model reduction})
        
        \mtskip
    Standard approximation strategies for Gaussian Process Regression may be linked with optimal spline approximation within the framework of a reproducing kernel Hilbert space determined by a given covariance function associated with a Bayesian prior.   We extend this framework to obtain optimal rational approximants (modeling rational transfer functions) utilizing intrinsic Gaussian processes and Bayesian priors that allow for data-driven modeling of dynamical systems with the management of nonstationary temporal dependence, drift, and observation errors.  I will review elements of a basic framework for Gaussian Process Regression and a version of ``rational kriging'' recently introduced by V. R. Joseph,  connecting this to optimal approximation in reproducing kernel Hilbert spaces with rational kernels, paralleling traditional Gaussian Process Regression approaches.
\end{ilasabstract}
    \newpage

\section{Abstracts of Contributed Talks}
\newpage

\end{document}
